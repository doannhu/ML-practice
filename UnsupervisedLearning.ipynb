{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When new samples are presented to a K-means model that already fitted old samples, this model remembers means of each cluster, called \"centroids\". Then new samples are assigned to the cluster whose centroid is closest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering 2D points\n",
    "\n",
    "From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you'll obtain the cluster labels for some new points using the .predict() method.\n",
    "\n",
    "You are given the array points from the previous exercise, and also an array new_points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(points)\n",
    "\n",
    "# Determine the cluster labels of new_points: labels\n",
    "labels = model.predict(new_points)\n",
    "\n",
    "# Print cluster labels of new_points\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1 2 0 1 2 1 2 2 2 0 1 2 2 0 0 2 0 0 2 2 0 2 1 2 1 0 2 0 0 1 1 2 2 2 0 1 2\n",
    " 2 1 2 0 1 1 0 1 2 0 0 2 2 2 2 0 0 1 1 0 0 0 1 1 2 2 2 1 2 0 2 1 0 1 1 1 2\n",
    " 1 0 0 1 2 0 1 0 1 2 0 2 0 1 2 2 2 1 2 2 1 0 0 0 0 1 2 1 0 0 1 1 2 1 0 0 1\n",
    " 0 0 0 2 2 2 2 0 0 2 1 2 0 2 1 0 2 0 0 2 0 2 0 1 2 1 1 2 0 1 2 1 1 0 2 2 1\n",
    " 0 1 0 2 1 0 0 1 0 2 2 0 2 0 0 2 2 1 2 2 0 1 0 1 1 2 1 2 2 1 1 0 1 1 1 0 2\n",
    " 2 1 0 1 0 0 2 2 2 1 2 2 2 0 0 1 2 1 1 1 0 2 2 2 2 2 2 0 0 2 0 0 0 0 2 0 0\n",
    " 2 2 1 0 1 1 0 1 0 1 0 2 2 0 2 2 2 0 1 1 0 2 2 0 2 0 0 2 0 0 1 0 1 1 1 2 0\n",
    " 0 0 1 2 1 0 1 0 0 2 1 1 1 0 2 2 2 1 2 0 0 2 1 1 0 1 1 0 1 2 1 0 0 0 0 2 0\n",
    " 0 2 2 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect your clustering\n",
    "\n",
    "Let's now inspect the clustering you performed in the previous exercise!\n",
    "\n",
    "A solution to the previous exercise has already run, so new_points is an array of points and labels is the array of their cluster labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Import matplotlib.pyplot as plt.\n",
    "    Assign column 0 of new_points to xs, and column 1 of new_points to ys.\n",
    "    Make a scatter plot of xs and ys, specifying the c=labels keyword arguments to color the points by their cluster label. Also specify alpha=0.5.\n",
    "    Compute the coordinates of the centroids using the .cluster_centers_ attribute of model.\n",
    "    Assign column 0 of centroids to centroids_x, and column 1 of centroids to centroids_y.\n",
    "    Make a scatter plot of centroids_x and centroids_y, using 'D' (a diamond) as a marker by specifying the marker parameter. Set the size of the markers to be 50 using s=50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "xs = new_points[:,0]\n",
    "ys = new_points[:,1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs,ys,c=labels,alpha=0.5)\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter(centroids_x,centroids_y,marker='D',s=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to measure clustering quality (metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good clustering has tight clusters, meaning that samples in each cluster are bunched together, not spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lower intertia is better\n",
    "- Not too many clusters\n",
    "\n",
    "Using elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Kmeans\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(samples)\n",
    "print(model.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters of grain?\n",
    "\n",
    "In the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You are given an array samples containing the measurements (such as area, perimeter, length, and several others) of samples of grain. What's a good number of clusters in this case?\n",
    "\n",
    "KMeans and PyPlot (plt) have already been imported for you.\n",
    "\n",
    "This dataset was sourced from the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    For each of the given values of k, perform the following steps:\n",
    "    Create a KMeans instance called model with k clusters.\n",
    "    Fit the model to the grain data samples.\n",
    "    Append the value of the inertia_ attribute of model to the list inertias.\n",
    "    The code to plot ks vs inertias has been written for you, so hit submit to see the plot!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model=KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the grain clustering with Crosstab -- **IMPORTANT**: using this method only when result is known\n",
    "\n",
    "In the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: \"Kama\", \"Rosa\" and \"Canadian\". In this exercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation.\n",
    "\n",
    "You have the array samples of grain samples, and a list varieties giving the grain variety for each sample. Pandas (pd) and KMeans have already been imported for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Create a KMeans model called model with 3 clusters.\n",
    "    Use the .fit_predict() method of model to fit it to samples and derive the cluster labels. Using .fit_predict() is the same as using .fit() followed by .predict().\n",
    "    Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.\n",
    "    Use the pd.crosstab() function on df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label. Assign the result to ct.\n",
    "    Hit submit to see the cross-tabulation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KMeans model with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "labels = model.fit_predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result:\n",
    "\n",
    "varieties  Canadian wheat  Kama wheat  Rosa wheat\n",
    "labels                                           \n",
    "0                       0           1          60\n",
    "1                      68           9           0\n",
    "2                       2          60          10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranforming features for better clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "    - If the data points are close to the mean, the variance is low.\n",
    "\n",
    "    - If the data points are spread out from the mean, the variance is high.\n",
    "\n",
    "    - Variance can be influenced heavily by outliers (extreme values that differ significantly from the rest of the data).\n",
    "    \n",
    "    - Variance is in the square of the units of the original data. For example, if the data is measured in meters, the variance will be in square meters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | Variance |\n",
    "|---------|----------|\n",
    "| Alcohol | 0.65     |\n",
    "| Malic_ac| 1.24     |\n",
    "....\n",
    "| od280   | 0.50     |\n",
    "| Proline | 99166.71 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proline has high variance. It affects the result of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(samples)\n",
    "\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "samples_scaled = scaler.transform(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "pipeline.fit(samples)\n",
    "\n",
    "pipeline.predict(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using Standard scaler:\n",
    "| Labels  | Barbera  | Barolo | Grignolino |\n",
    "|---------|----------|--------|------------|\n",
    "| 0       | 29       | 13     | 20         |\n",
    "| 1       | 0        | 46     | 1          |\n",
    "| 2       | 19       | 0      | 50         |\n",
    "\n",
    "These clusters weren't good because each label should contain single type. For example, label should only contain Barolo type or mostly contain Barolo. \n",
    "\n",
    "\n",
    "After using standardization\n",
    "\n",
    "| Labels  | Barbera  | Barolo | Grignolino |\n",
    "|---------|----------|--------|------------|\n",
    "| 0       | 0        | 59     | 3          |\n",
    "| 1       | 48       | 0      | 3          |\n",
    "| 2       | 0        | 0      | 65         |\n",
    "\n",
    "These clusters are better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** other preprocessing methods: Normalization and MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxAbsScaler for **non-normal distribution**\n",
    "Ex: \n",
    "Original data:\n",
    " [[1. 2. 3.]\n",
    " [4. 5. 6.]\n",
    " [7. 8. 9.]]\n",
    "\n",
    "Scaled data:\n",
    " [[0.14285714 0.25       0.33333333]\n",
    " [0.57142857 0.625      0.66666667]\n",
    " [1.         1.         1.        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the data normal distribution or non normal distribution, using **Anderson-Darling test** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import anderson\n",
    "\n",
    "# Generate some example data (non-normally distributed)\n",
    "data = np.random.exponential(size=100)\n",
    "\n",
    "# Perform the Anderson-Darling test\n",
    "result = anderson(data)\n",
    "\n",
    "print(\"Statistic:\", result.statistic)\n",
    "print(\"Critical Values:\", result.critical_values)\n",
    "print(\"Significance Levels:\", result.significance_level)\n",
    "\n",
    "# Interpret the result\n",
    "print(\"\\nInterpretation:\")\n",
    "if result.statistic < result.critical_values[2]:\n",
    "    print(\"Data looks normal at the 5% significance level.\")\n",
    "else:\n",
    "    print(\"Data does not look normal at the 5% significance level.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the main differences between StandardScaler and Normalizer are:\n",
    "\n",
    "**Purpose**: StandardScaler aims to bring features to a standard scale with mean 0 and standard deviation 1, while Normalizer scales the magnitude of individual samples to 1.\n",
    "\n",
    "**Application**: StandardScaler is often used when features have different scales, and Normalizer is used when you want to emphasize the direction of data points.\n",
    "\n",
    "**Effect**: StandardScaler retains information about the distribution and spread of the data, while Normalizer emphasizes the relative relationships between data points.\n",
    "\n",
    "**Direction**: StandardScaler operates column-wise, and Normalizer operates row-wise.\n",
    "\n",
    "**StandardScaler** is typically applied column-wise, which means each feature is scaled independently of the others.\n",
    "\n",
    "**Normalizer** is applied row-wise, treating each sample as a **vector** and scaling its magnitude to 1 while preserving the direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF STANDARDLIZATION\n",
    "\n",
    "# example of a standardization\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# define data\n",
    "data = asarray([[100, 0.001],\n",
    "                 [8, 0.05],\n",
    "                 [50, 0.005],\n",
    "                 [88, 0.07],\n",
    "                 [4, 0.1]])\n",
    "print(data)\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[1.0e+02 1.0e-03]\n",
    " [8.0e+00 5.0e-02]\n",
    " [5.0e+01 5.0e-03]\n",
    " [8.8e+01 7.0e-02]\n",
    " [4.0e+00 1.0e-01]]\n",
    "\n",
    "[[ 1.26398112 -1.16389967]\n",
    " [-1.06174414  0.12639634]\n",
    " [ 0.         -1.05856939]\n",
    " [ 0.96062565  0.65304778]\n",
    " [-1.16286263  1.44302493]]\n",
    "\n",
    " **Note** assume the data has normal distribution, old mean is 50 for 1st feature, then new mean is 0. For the 2nd feature, the old mean is 0.0452 and the new mean is 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
